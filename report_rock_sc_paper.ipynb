{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로를 이용한 가위바위보 분류기 만들기 \n",
    "\n",
    "텐서플로의 Sequential API를 이용하여 쉽게 가위바위보 분류기 만들기 \n",
    "\n",
    "\n",
    "### 데이터 만들기 \n",
    "\n",
    "카메라만 연결되어있다면 구글 teachable machine에서 쉽게 이미지 데이터를 만들수있다.\n",
    "가위, 바위, 보 각각 촬영하고 본인이 원하는 경로에 따로따로 폴더를 만들어 저장하자. \n",
    "[링크 ](https://teachablemachine.withgoogle.com/)\n",
    "\n",
    "찍을 때 고려하면 좋은점들(과 그러면 해결할수있는 문제들)\n",
    "\n",
    "1. 되도록 여러 각도에서 찍는게 좋음(Viewpoint Variation) \n",
    "2. 여러 크기로 찍는 것이 좋음(Deformation)\n",
    "3. 여러 사람의 손을 찍는 것이 좋음(inttraclass Variation)\n",
    "4. 여러 밝기로 촬영해보는것도 좋음(illumination changes)\n",
    "5. 다른 배경을 바탕으로 찍어보는 것도 좋음(Background clutter)\n",
    "\n",
    "필자는 가위 바위 보 각각 1000개의 이미지를 동료들과 함께 만들었다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "\n",
    "#### 이미지 사이즈 변경\n",
    "사진들을 열심히 찍고 다운로드를 마친다면 이미지 파일들이 224x224 pixel인것을 확인할 수있다. 이것으로 분석해도 괜찮지만 우리들의 컴퓨터 파워는 그렇게 좋지않을테니 이미지 파일을 28x28 단위로 resize 하기로 하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "# PIL library가 설치되어있지 않은 분은 설치\n",
    "#!pip install pillow \n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 각자 자기 파일 폴더로 \n",
    "image_dir_path = \"/home/ssac4/aiffel/report/rock_scissor_paper/train\"\n",
    "\n",
    "# 가위 폴더 \n",
    "images=glob.glob(image_dir_path + \"/scissor/*.jpg\")  \n",
    "\n",
    "# 가위 폴더의 이미지 파일 모두 28x28 사이즈로 바꾸어 저장. \n",
    "target_size=(28,28)\n",
    "for img in images:\n",
    "    old_img=Image.open(img)\n",
    "    new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "    new_img.save(img,\"JPEG\")\n",
    "\n",
    "print(\"보 이미지 resize 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 바위 폴더 경로 \n",
    "images=glob.glob(image_dir_path + \"/rock/*.jpg\")  \n",
    "\n",
    "# 바위 폴더 이미지 파일 모두 28x28 사이즈로 바꾸어 저장. \n",
    "target_size=(28,28)\n",
    "for img in images:\n",
    "    old_img=Image.open(img)\n",
    "    new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "    new_img.save(img,\"JPEG\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보 폴더 \n",
    "images=glob.glob(image_dir_path + \"/paper/*.jpg\")  \n",
    "\n",
    "# 보 폴더 이미지 파일 모두 28x28 사이즈로 바꾸어 저장. \n",
    "target_size=(28,28)\n",
    "for img in images:\n",
    "    old_img=Image.open(img)\n",
    "    new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "    new_img.save(img,\"JPEG\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 라벨링\n",
    "우리가 가진 데이터에는 아직 이미지 데이터가 가위인지 바위인지 보인지에 대한 라벨링이 되어있지않다. 가위바위보는 3가지 경우이기때문에 가위는 0, 바위는 1, 보는 2라는 정보를 추가해서 라벨링해야한다.\n",
    "\n",
    "아래 함수를 통해서 불러오고 라벨링하자 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 3000 입니다.\n",
      "x_train shape: (3000, 28, 28, 3)\n",
      "y_train shape: (3000,)\n"
     ]
    }
   ],
   "source": [
    "# img_path는 파일경로. 아까 입력한 경로 그대로 쓰면된다. \n",
    "# number_of_data는 입력할 train 데이터를 적으면된다. 필자는 3000이다(3*1000)\n",
    "def load_data(img_path,number_of_data):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_dir_path = \"/home/ssac4/aiffel/report/rock_scissor_paper/train\"\n",
    "(x_train, y_train)=load_data(image_dir_path,3000)\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0 가위\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0])\n",
    "\n",
    "def ex_label(a):\n",
    "    if a == 0:\n",
    "        return \"가위\"\n",
    "    elif a == 1:\n",
    "        return \"바위\"\n",
    "    else:\n",
    "        return \"보\" \n",
    "\n",
    "\n",
    "\n",
    "print('라벨: ', y_train[0] , ex_label(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28x28 픽셀의 이미지다 보니 상상이상으로 나쁘다.      \n",
    "우린 참 훌륭한 세상에 살고 있었던 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 네트워크 설계 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  7\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_50 (Conv2D)           (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 32)                25632     \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 30,819\n",
      "Trainable params: 30,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "random.seed(a= 100)\n",
    "\n",
    "\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "print('Model에 추가된 Layer 개수: ', len(model.layers))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 네트워크 학습 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 2.4281 - accuracy: 0.4270\n",
      "Epoch 2/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.9093 - accuracy: 0.5760\n",
      "Epoch 3/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.6723 - accuracy: 0.7063\n",
      "Epoch 4/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5003 - accuracy: 0.7980\n",
      "Epoch 5/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3890 - accuracy: 0.8493\n",
      "Epoch 6/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2987 - accuracy: 0.8840\n",
      "Epoch 7/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2584 - accuracy: 0.9020\n",
      "Epoch 8/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1917 - accuracy: 0.9293\n",
      "Epoch 9/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1570 - accuracy: 0.9433\n",
      "Epoch 10/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1157 - accuracy: 0.9577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feb901df690>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 데이터 입력하기 \n",
    "테스트 데이터는 모델 학습에 사용되지 않은 데이터를 사용해야한다.     \n",
    "그러므로 다시 아래 링크에서 이미지 데이터를 만든후 test 데이터를 놓아 따로 정리하자\n",
    "필자는 가위 바위 보 각각 100개씩 총 300개의 test 데이터를 만들었다. \n",
    "[링크 ](https://teachablemachine.withgoogle.com/)\n",
    "\n",
    "train 데이터와 같이 로드한 데이터를 정규화하도록 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 300 입니다.\n",
      "x_test shape: (300, 28, 28, 3)\n",
      "y_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "# 각자 test data를 모은 링크를 사용하자 \n",
    "image_dir_path = \"/home/ssac4/aiffel/report/rock_scissor_paper/test\"\n",
    "\n",
    "(x_test, y_test)=load_data(image_dir_path,300)\n",
    "\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 데이터로 모델 성능 확인해보기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 3.3978 - accuracy: 0.4567\n",
      "test_loss: 3.397765636444092 \n",
      "test_accuracy: 0.4566666781902313\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필자의 경우 0.45 약 45%의 정확도가 나타났다. 이를 더 좋게 만들 수 없을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 더 좋은 모델 만들어보기 \n",
    "\n",
    "### 데이터 정규화\n",
    "\n",
    "현재 컬러를 0부터 255까지로 표현하고 있는데 이를 최솟값 0, 최댓값 1로 정규화하는것이 모델의 성능을 상승시킨다. 그러므로 이미지 파일을 255로 나눈다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규화한 x_train_norm과 x_test_norm을 이용하여 모델을 적합시켜본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0128 - accuracy: 0.9973\n",
      "Epoch 2/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.9990\n",
      "Epoch 3/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0497 - accuracy: 0.9830\n",
      "Epoch 4/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0157 - accuracy: 0.9970\n",
      "Epoch 5/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feb8442ec10>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 3.5036 - accuracy: 0.6200\n",
      "test_loss: 3.503636121749878 \n",
      "test_accuracy: 0.6200000047683716\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_norm,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "약 62%의 정확도를 나타내게 되었다. 더 변경할 수 있는건 없을까?\n",
    "\n",
    "### 패러미터 변경 \n",
    "아까와 동일한 모델에서 패러미터들만 변경할수있게 변수로 변경하였다.\n",
    "n_channel_1는 첫번째 Conv2D 레이어가 사용하는 이미지 피쳐 숫자. 디테일하고 복잡한 이미징리수록 늘려주는걸 고려하면 좋다.  \n",
    "n_channel_2는 두번째 Conv2D 레이어가 사용하는 이미지 피쳐 숫자. 디테일하고 복잡한 이미징리수록 늘려주는걸 고려하면 좋다.  \n",
    "n_dense 는 분류기에 사용되는 뉴런의 숫자임. 일반적으로 분류해야할 종류가 많을수록 더 큰 숫자를 사용함. \n",
    "n_train_epoch는 훈련을 통해 모델을 피팅하는 행위인 epochl의 사용횟수이다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_100 (Conv2D)          (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_100 (MaxPoolin (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_101 (Conv2D)          (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_101 (MaxPoolin (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_50 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 72)                57672     \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 10)                730       \n",
      "=================================================================\n",
      "Total params: 68,546\n",
      "Trainable params: 68,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_channel_1=32  \n",
    "n_channel_2=32\n",
    "n_dense= 72\n",
    "n_train_epoch=10\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 1.1687 - accuracy: 0.3980\n",
      "Epoch 2/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.9397 - accuracy: 0.5547\n",
      "Epoch 3/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.7291 - accuracy: 0.6963\n",
      "Epoch 4/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5424 - accuracy: 0.7883\n",
      "Epoch 5/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.4224 - accuracy: 0.8440\n",
      "Epoch 6/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.8813\n",
      "Epoch 7/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2825 - accuracy: 0.8993\n",
      "Epoch 8/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2322 - accuracy: 0.9147\n",
      "Epoch 9/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1907 - accuracy: 0.9393\n",
      "Epoch 10/10\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1652 - accuracy: 0.9457\n",
      "10/10 - 0s - loss: 1.6601 - accuracy: 0.6367\n",
      "test_loss: 1.6601307392120361 \n",
      "test_accuracy: 0.6366666555404663\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "model.fit(x_train_norm, y_train, epochs=n_train_epoch)\n",
    "\n",
    "# 모델 시험\n",
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "63%으로 미세한 성능의 증가가 있었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회상 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 좋은 데이터 셋을 구하기는 힘들다는걸 느꼈음.\n",
    "\n",
    "웹캠을 통해서 찍다보니 배경같은것들이 통일되지 않아 부족함 \n",
    "\n",
    "##### 2. 데이터 정규화만해도 모델의 성능이 어느정도 개선이 되는 점에 대해서 놀라웠음\n",
    "\n",
    "대략 45%의 정확성에서 50~60%까지 정확성이 상승하는 점이 신기했음 \n",
    "\n",
    "##### 3. 모델의 성능이 아직 매 횟수마다 달라지는 점이 있어서 부족한 점이 있음. \n",
    "\n",
    "tf.random.set_seed의 명령어를 사용해서 고정시켜보려했는데 잘 되지않았음.\n",
    "좀 더 공부가 필요함 \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
